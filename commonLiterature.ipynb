{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets \n# preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-13T14:11:06.4598Z","iopub.execute_input":"2021-07-13T14:11:06.460164Z","iopub.status.idle":"2021-07-13T14:11:06.471486Z","shell.execute_reply.started":"2021-07-13T14:11:06.460135Z","shell.execute_reply":"2021-07-13T14:11:06.47025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nimport time\n\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:06.499806Z","iopub.execute_input":"2021-07-13T14:11:06.500176Z","iopub.status.idle":"2021-07-13T14:11:06.511162Z","shell.execute_reply.started":"2021-07-13T14:11:06.500144Z","shell.execute_reply":"2021-07-13T14:11:06.510154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/input/commonlitreadabilityprize\"\ntrain_path = os.path.join(train_dir, \"train.csv\")\ntest_path = os.path.join(train_dir, \"test.csv\")\n\nsample = os.path.join(train_dir, \"sample_submission.csv\")\ndf = pd.read_csv(train_path)\ndf.head()\n\ndf_test = pd.read_csv(test_path)\n\ndf_sample = pd.read_csv(sample)\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:06.524091Z","iopub.execute_input":"2021-07-13T14:11:06.524492Z","iopub.status.idle":"2021-07-13T14:11:06.583199Z","shell.execute_reply.started":"2021-07-13T14:11:06.52446Z","shell.execute_reply":"2021-07-13T14:11:06.582181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graph(data):\n    tar_s = np.unique(data['target']).shape\n\n    fig,ax = plt.subplots(1,2,figsize=(15,8))\n    sns.scatterplot(x = np.arange(tar_s[0]), y =data['target'],ax=ax[0] )\n    sns.scatterplot(x = np.arange(tar_s[0]), y =data['standard_error'],ax=ax[1] )\n    return fig, ax\nplot_graph(df)\n    # A outlier could easily be drop with IQR and pandas dataframe filter method\n    # There is one outlier in standard error at index = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:06.584658Z","iopub.execute_input":"2021-07-13T14:11:06.58495Z","iopub.status.idle":"2021-07-13T14:11:07.027167Z","shell.execute_reply.started":"2021-07-13T14:11:06.584921Z","shell.execute_reply":"2021-07-13T14:11:07.026139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_validate_data(X, y):\n    X_train, X_test, y_train,y_test = train_test_split(X, y, test_size= 0.2)\n    print(X_train.shape, X_test.shape, y_train.shape,y_test.shape)\n    return X_train, X_test, y_train,y_test\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:07.029234Z","iopub.execute_input":"2021-07-13T14:11:07.029592Z","iopub.status.idle":"2021-07-13T14:11:07.035636Z","shell.execute_reply.started":"2021-07-13T14:11:07.029559Z","shell.execute_reply":"2021-07-13T14:11:07.034392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Q3 = df_describe.loc['75%']; Q1 = df_describe.loc['25%']\nIQR = Q3 - Q1\n\nupper_iqr = Q3 +1.5*IQR\nlower_iqr  = Q1 - 1.5*IQR\n\n# print(f\"Condition for outliers is {df['target']<=  upper_iqr and df['target']<= lower_iqr}\")\ndf['target']\nm = map(lambda x: x>= lower_iqr, df['target'])\n\n\ndf_upper = df.groupby(\"target\").filter(lambda x: (x['standard_error']> upper_iqr['standard_error'] ))\ndf_lower = df.groupby(\"target\").filter(lambda x: (x['standard_error']< lower_iqr['standard_error'] ))\n\ndf_upper2 = df.groupby(\"standard_error\").filter(lambda x: (x['target']> upper_iqr['target'] ))\ndf_lower2 = df.groupby(\"standard_error\").filter(lambda x: (x['target']< lower_iqr['target'] ))\n\ndf.drop(index= df_upper.index, axis=0, inplace=True)\ndf.drop(index = df_lower.index, axis=0, inplace=True)\ndf.drop(index = df_upper2.index, axis=0, inplace=True)\ndf.drop(index = df_lower2.index, axis=0, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:07.037215Z","iopub.execute_input":"2021-07-13T14:11:07.037529Z","iopub.status.idle":"2021-07-13T14:11:10.135609Z","shell.execute_reply.started":"2021-07-13T14:11:07.037504Z","shell.execute_reply":"2021-07-13T14:11:10.13419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graph(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:10.137163Z","iopub.execute_input":"2021-07-13T14:11:10.13751Z","iopub.status.idle":"2021-07-13T14:11:10.501762Z","shell.execute_reply.started":"2021-07-13T14:11:10.137486Z","shell.execute_reply":"2021-07-13T14:11:10.500148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(model, X, y):\n    y_pred = model.predict(X)\n    score = cross_val_score(model, X, y, n_jobs= -1, cv= 5); m = mean_squared_error(y, y_pred)\n    print(f\"Cross_val_Score for X_train prediction:\\t{score}\\nmean squared error:\\t{m}\")\n    return score, m ","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:10.503935Z","iopub.execute_input":"2021-07-13T14:11:10.504287Z","iopub.status.idle":"2021-07-13T14:11:10.510044Z","shell.execute_reply.started":"2021-07-13T14:11:10.504258Z","shell.execute_reply":"2021-07-13T14:11:10.509195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\npat = re.compile(\"!@#$%^&*()_+<>[?]\")\ndef pre(data):\n    data =data.replace(\"!@#$%^&*()_+<>[?]\",\"\")\n    return data\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words='english')\nd = vectorizer.fit_transform(df['excerpt'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:10.511329Z","iopub.execute_input":"2021-07-13T14:11:10.511614Z","iopub.status.idle":"2021-07-13T14:11:11.168377Z","shell.execute_reply.started":"2021-07-13T14:11:10.511573Z","shell.execute_reply":"2021-07-13T14:11:11.166772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = df['target']\nX_train = d.toarray()\nX_train.shape, y_train.shape\n\nX_train, X_test, y_train,y_test = get_validate_data(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:11.170578Z","iopub.execute_input":"2021-07-13T14:11:11.170818Z","iopub.status.idle":"2021-07-13T14:11:11.487869Z","shell.execute_reply.started":"2021-07-13T14:11:11.170796Z","shell.execute_reply":"2021-07-13T14:11:11.486835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.gaussian_process import GaussianProcessRegressor\n\nmodel = GaussianProcessRegressor(n_restarts_optimizer= 10 )\n\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:11.489278Z","iopub.execute_input":"2021-07-13T14:11:11.489532Z","iopub.status.idle":"2021-07-13T14:11:54.659385Z","shell.execute_reply.started":"2021-07-13T14:11:11.48951Z","shell.execute_reply":"2021-07-13T14:11:54.657552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score, test_mean = rmse_score(model, X_test,y_test)\ntrain_score, train_mean = rmse_score(model, X_train,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:16:41.946425Z","iopub.execute_input":"2021-07-13T14:16:41.946779Z","iopub.status.idle":"2021-07-13T14:17:03.493006Z","shell.execute_reply.started":"2021-07-13T14:16:41.946749Z","shell.execute_reply":"2021-07-13T14:17:03.491277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize= (15, 8))\nax[0].plot(test_score, 'y')\nax[0].plot(train_score, 'r')\nax[0].set_title(\"cross_val_score\")\n\nax[0].annotate(\"Validation data\",xy= (2.0,0.410), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\nax[0].annotate(\"training data\",xy= (2.0,0.30), arrowprops=dict(facecolor='red', shrink=5),fontsize=16,horizontalalignment='left')\n\n# from graph, it is clear, our model is overfitting data very high, could be fix with l1 or l2 ","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.66176Z","iopub.status.idle":"2021-07-13T14:11:54.662268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gradient boosting regression \nx = time.time()\nparams = {'n_estimators': 500,\n          'max_depth': 4,\n          'min_samples_split': 5,\n          'learning_rate': 0.01,\n          'loss': 'ls'\n         }\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngrad_estimator = GradientBoostingRegressor(**params)\n\ngrad_estimator.fit(X_train, y_train)\n\ng_score, g_mean = rmse_score(grad_estimator, X_test, y_test)\n\nprint(\"completion time\")\nprint(time.time()-x)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:18:27.406977Z","iopub.execute_input":"2021-07-13T14:18:27.407316Z","iopub.status.idle":"2021-07-13T14:39:04.652697Z","shell.execute_reply.started":"2021-07-13T14:18:27.40727Z","shell.execute_reply":"2021-07-13T14:39:04.651402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_score_train, g_mean_train = rmse_score(grad_estimator, X_train, y_train)\ng_score,g_mean","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:39:04.654578Z","iopub.execute_input":"2021-07-13T14:39:04.654841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\nfor i, y_pred in enumerate(grad_estimator.staged_predict(X_test)):\n    test_score[i] = grad_estimator.loss_(y_test, y_pred)\n\nfig = plt.figure(figsize=(6, 6))\nplt.subplot(1, 1,1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, grad_estimator.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\nfig.tight_layout()\nfig","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.666085Z","iopub.status.idle":"2021-07-13T14:11:54.666652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_from_gauss(data, num):\n    p = data.iloc[num]['excerpt']\n\n    p = vectorizer.transform([p])\n    pred = model.predict(p.toarray())\n    print(pred)\n    return pred[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.667569Z","iopub.status.idle":"2021-07-13T14:11:54.668127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_score, g_mean","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.669034Z","iopub.status.idle":"2021-07-13T14:11:54.669601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['target']=0\n\nfor i in range(df_test.shape[0]):\n    df_test.iloc[i, -1] = predict_from_gauss(df_test, i)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.670517Z","iopub.status.idle":"2021-07-13T14:11:54.671088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ['url_legal','license','excerpt']:\n    try:\n        df_test.drop(i, inplace=True, axis=1)\n    except KeyError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.671993Z","iopub.status.idle":"2021-07-13T14:11:54.67257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:11:54.673447Z","iopub.status.idle":"2021-07-13T14:11:54.673988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}